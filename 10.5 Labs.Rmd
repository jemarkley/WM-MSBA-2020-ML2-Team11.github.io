---
title: "10.5 Lab: K-Means and Hierarchical Clustering"
author: "Team 11"
date: "3/19/2020"
output: html_document
---
### 10.5.1 K-Means Clustering

to begin, a 50x2 matrix is populated with random normally distributed values centered at 0. The means of the first 25 rows are then shifted to create 2 artificial clusters in the data.

```{r}
set.seed(2)
x=matrix(rnorm(50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
```

next, an initial model is fitted using the kmeans() function. the 3 arguments passed to the fuction are the dataset, the number of clusters desired (k), and the number of intitial random starting points desired.
 
```{r}
km.out=kmeans(x,2,nstart=20)
```

cluster assignments for each row of data can be shown by calling km.out$cluster. Assignments for this model can be seen below.

```{r}
km.out$cluster
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)
```

While we know that our dataset was build to represent 2 clusters, lets see what the algorithm does when we set k=3.
```{r}
set.seed(4)
km.out=kmeans(x,3,nstart=20)
km.out
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=3", xlab="", ylab="", pch=20, cex=2)
```

Additionally, we can highlight the importance of the nstart argument by comparing a model built on 1 initial starting point as opposed to one built with 20.

nstart = 1:
```{r}
set.seed(7)
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
```

nstart = 20:
```{r}
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
```
Notice how the model with 20 different random starting points performs better that the one with only 1. As nstart increases, we become more likely to create a model with optimal splits.

### 10.5.2 Heirarchical Clustering

For heirarchical clustering, the hclust() function is used, which takes two arguments, a matrix of all inter-observation distances, and the clustering mehod desired. Below, 3 models are fit for euclidean distance using complete (farthest 2 points), average (mean of all possible point distances), and single linkage (closest 2 points) clustering.
```{r}
hc.complete=hclust(dist(x), method="complete")
hc.average=hclust(dist(x), method="average")
hc.single=hclust(dist(x), method="single")
```
Next, dendograms for each model are plotted. Note that the numbers at the bottom of the plot are representative of each observation with the height representing the euclidean distance between clusters.
```{r}
par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=.9)
```

The cutree() function shows all cluster assignments in order at a given stopping point. In this case, all 3 models are stopped once 2 clusters remain. Notice how in the hc.single model, nearly all observations are assigned to cluster 1 with only one being assigned to cluster 2. This is a flaw of the single linkage method, as it is extremely sensitive to outliers.
```{r}
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
```

Here is an example of the cutree() function stopped at 4 clusters for single linkage. Note how now clusters 2 and 4 only hold 1 observation each.
```{r}
cutree(hc.single, 4)
```

Feature scaling before heirarchical clustering is very common and often needed for datasets with features recorded in different units
```{r}
xsc=scale(x)
plot(hclust(dist(xsc), method="complete"), main="Hierarchical Clustering with Scaled Features")
```
Correlation-based distance can be computed using the as.dist() funcas.dist() tion, which converts an arbitrary square symmetric matrix into a form that the hclust() function recognizes as a distance matrix. However, this only makes sense for data with at least three features since the absolute correlation between any two observations with measurements on two features is always 1. Hence, we will cluster a three-dimensional data set.

```{r}
x=matrix(rnorm(30*3), ncol=3)
dd=as.dist(1-cor(t(x)))
plot(hclust(dd, method="complete"), main="Complete Linkage with Correlation-Based Distance", xlab="", sub="")
```
